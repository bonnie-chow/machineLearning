{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imdb_text_cnn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "Gnv_1eVDe2xw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "01b264a6-9537-4266-adfc-59a5d8bade4e"
      },
      "cell_type": "code",
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D, MaxPooling1D, Flatten, GlobalAveragePooling1D\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Model\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers \\\n",
        "\n",
        "from fastai.vision import *\n",
        "    \n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZHVHibQ2e2yJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Download data and create dataframe\n",
        "path = untar_data(URLs.IMDB_SAMPLE)\n",
        "df = pd.read_csv(path/'texts.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gzfN6avte2yU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2d2619f0-91be-452e-b379-3581f86ab2ef"
      },
      "cell_type": "code",
      "source": [
        "# Shuffle the indices randomly\n",
        "df = df.iloc[np.random.permutation(len(df))]\n",
        "print(df.head(10))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        label                                               text  is_valid\n",
            "177  positive  This was the best documentary I've ever seen!!...     False\n",
            "614  negative  A below average looking video game is turned i...     False\n",
            "390  positive  Sure, it's a 50's drive-in special, but don't ...     False\n",
            "298  positive  This is a very dramatic and suspenseful movie....     False\n",
            "282  positive  If in the 90's you're adapting a book written ...     False\n",
            "916  negative  I had suspicions the movie was going to be bad...      True\n",
            "543  negative  I saw this film in its premier week in 1975. I...     False\n",
            "770  negative  This was the first Ewan McGregor movie I ever ...     False\n",
            "223  positive  A true dark noir movie and a very graphic film...     False\n",
            "499  positive  A very realistic portrait of a broken family a...     False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fKb5YmSQe2yc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "f6a9e54f-87ca-4dd6-cc24-9ac39fb48a80"
      },
      "cell_type": "code",
      "source": [
        "# Replace string value with integers\n",
        "df['label'] = df['label'].map({'negative':0, 'positive':1})\n",
        "print(df.head())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     label                                               text  is_valid\n",
            "177      1  This was the best documentary I've ever seen!!...     False\n",
            "614      0  A below average looking video game is turned i...     False\n",
            "390      1  Sure, it's a 50's drive-in special, but don't ...     False\n",
            "298      1  This is a very dramatic and suspenseful movie....     False\n",
            "282      1  If in the 90's you're adapting a book written ...     False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "htjANthne2ym",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "VALIDATION_PERCENT = 0.2\n",
        "EMBED_SIZE = 100 # size of word vector\n",
        "MAX_FEATURES = 12000 # number of unique words to use (i.e num rows in embedding vector)\n",
        "MAX_LEN = 200 # max number of words in a review\n",
        "NUMBER_OF_CLASSES = 1 # output of model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ol1brLyre2ys",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "adeb5d07-3532-4735-aebd-ae7c8965386b"
      },
      "cell_type": "code",
      "source": [
        "# Split into training and validation sets\n",
        "cut = int(VALIDATION_PERCENT * len(df)) + 1\n",
        "train_df, valid_df = df[cut:], df[:cut]\n",
        "\n",
        "print(train_df.head())\n",
        "print(valid_df.head())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     label                                               text  is_valid\n",
            "544      1  Hmmmm, want a little romance with your mystery...     False\n",
            "716      1  I adored this movie. Not only because I am a b...     False\n",
            "765      0  Felt mine was while watching this...but it see...     False\n",
            "903      1  After high-school graduation, best friends Ali...      True\n",
            "496      1  this independent film was one of the best film...     False\n",
            "     label                                               text  is_valid\n",
            "177      1  This was the best documentary I've ever seen!!...     False\n",
            "614      0  A below average looking video game is turned i...     False\n",
            "390      1  Sure, it's a 50's drive-in special, but don't ...     False\n",
            "298      1  This is a very dramatic and suspenseful movie....     False\n",
            "282      1  If in the 90's you're adapting a book written ...     False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Whv93y8qe2yz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Fill missing values with _na_\n",
        "train_X = train_df[\"text\"].fillna(\"_na_\").values\n",
        "valid_X = valid_df[\"text\"].fillna(\"_na_\").values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dyH2-N0Be2y5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Tokenize the sentences\n",
        "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
        "tokenizer.fit_on_texts(list(train_X))\n",
        "train_X = tokenizer.texts_to_sequences(train_X)\n",
        "valid_X = tokenizer.texts_to_sequences(valid_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yGX9PJVJe2zA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "979ea69a-dc89-4916-ec0f-88f6c52be666"
      },
      "cell_type": "code",
      "source": [
        "print('Train_X: ')\n",
        "print(train_X[0:5])\n",
        "\n",
        "print('Tokenizer: ')\n",
        "word_index = tokenizer.word_index\n",
        "print('# of unique tokens: ', len(word_index))\n",
        "print(list(tokenizer.index_word.items())[0:10])\n",
        "print(list(tokenizer.index_word.items())[-10:])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train_X: \n",
            "[[4689, 162, 3, 119, 665, 17, 130, 945, 10, 46, 9, 11, 103, 43, 1, 665, 13, 4690, 10, 64, 25, 93, 14, 3, 114, 15, 18, 88, 101, 1, 665, 26, 4690, 53, 1, 3812, 2749, 35, 135, 429, 3, 9012, 665, 518, 822, 1, 2195, 12, 6, 66, 1, 665, 83, 4690, 1, 60, 206, 25, 83, 3, 119, 95, 642, 1, 945, 120, 297, 95, 73, 161, 235, 18, 163, 20, 1, 625, 4, 1479, 1019, 3178, 6137, 438, 72, 11, 198, 3813, 1827, 13, 165, 51, 40, 439, 3814, 11, 103, 1480, 45, 302, 1303, 18, 10, 3179, 3180, 254, 207, 101, 25, 83, 114, 204, 294, 17, 3, 49, 72, 588, 255, 15, 278, 1, 981, 13, 414, 2, 1, 415, 101, 25, 83, 2750, 20, 178, 101, 25, 83, 3, 119, 9013, 3, 119, 4691, 49, 4692, 27, 151, 12, 13, 89, 38, 10, 13, 1, 394, 4, 148, 2196, 1481, 12, 101, 742, 25, 83, 1482, 36, 10, 101, 25, 83, 666, 8, 6138, 18, 31, 62, 59, 8, 9014, 319, 1384, 34, 9015, 782, 3, 9016, 13, 1, 195, 2, 39, 9017, 1692, 5, 2197, 879, 589, 395, 3815, 495, 22, 51, 66, 10, 83, 3, 49, 847, 1483, 395, 287, 14, 1, 193, 263, 395, 3815, 495, 124, 62, 336, 5, 220, 3, 9018, 2, 41, 26, 3, 367, 164, 4, 1, 1304, 10, 13, 41, 3, 255, 15, 146, 36, 31, 66, 5, 783, 45, 6139, 5, 1, 395, 2, 45, 4, 12, 6139, 401, 35, 1, 1574, 396, 9019, 62, 3, 73, 15, 590, 387, 23, 48, 10, 13, 421, 1, 125, 4693, 4694, 295, 11, 25, 131, 112, 2, 1, 61, 12, 11, 121, 4, 11, 9020, 6140, 10, 5, 26, 3, 1305, 93, 14, 255, 15, 946, 9, 3, 982, 4, 1063], [11, 3816, 10, 15, 22, 61, 79, 11, 288, 3, 193, 388, 4, 9021, 6141, 266, 28, 6, 8, 2751, 30, 848, 104, 12, 1385, 18, 77, 79, 28, 6, 22, 1, 311, 278, 1, 430, 6, 607, 115, 32, 9022, 4695, 37, 13, 1, 61, 330, 5, 105, 9023, 9024, 2, 37, 62, 3817, 10, 15, 20, 24, 4696, 7, 7, 3, 209, 1064, 35, 2752, 5, 1828, 3818, 1, 848, 880, 10, 6, 67, 185, 6142, 2, 2753, 4, 548, 3819, 8, 2754, 2, 2198, 6143, 201, 31, 118, 52, 784, 3, 3181, 2, 1, 1575, 1, 15, 276, 5, 10, 183, 528, 6, 36, 4697, 2, 29, 1, 160, 55, 153, 2, 1306, 35, 10, 183, 9025, 1, 60, 486, 6, 1386, 12, 4, 1, 91, 1020, 4, 10, 209, 6144, 6141, 2, 6145, 4695, 6146, 880, 5, 397, 3, 2199, 6, 3182, 32, 9026, 17, 24, 549, 2, 24, 3183, 3184, 5, 1065, 5, 2752, 48, 983, 6, 3, 89, 2, 608, 3820, 166, 29, 48, 106, 68, 397, 3185, 32, 1, 3186, 23, 86, 7, 7, 8, 1, 118, 10, 15, 154, 129, 63, 106, 6146, 39, 9027, 13, 49, 1112, 39, 9028, 39, 49, 235, 9, 1232, 407, 179, 3, 4698, 1576, 29, 48, 9, 219, 26, 34, 5, 1693, 14, 1387, 53, 91, 2755, 2, 9029, 21, 643, 2, 10, 166, 6, 22, 2459, 32, 6147, 39, 4699, 5, 308, 626, 63, 175, 9, 136, 3, 89, 19, 14, 92, 2, 189, 626, 1577, 17, 1, 6148, 3187, 35, 9030, 1, 180, 12, 195, 9031, 9032, 1829, 5, 1828, 35, 3821, 8, 1, 4700, 77, 4701, 10, 15, 3, 765, 3188, 4, 82, 3189, 2, 1021, 4702, 1, 6149, 20, 1, 2756, 21, 823, 341, 32, 1, 389, 204, 283, 4695, 2, 10, 11, 62, 336, 5, 3190, 1, 119, 402, 37, 312, 199, 6145, 12, 9033, 279, 64, 26, 3, 226, 881, 5, 568, 32, 41, 38, 298, 89, 15, 138, 65, 9], [422, 1169, 13, 122, 145, 10, 18, 9, 173, 12, 6, 1, 330, 14, 9034, 785, 8, 1, 209, 8, 10, 19, 22, 12, 175, 3, 184, 4, 248, 477, 16, 320, 25, 824, 10, 13, 27, 4, 9035, 221, 107, 2, 42, 61, 24, 263, 1830, 12, 4701, 9, 92, 3189, 29, 30, 42, 743, 12, 30, 4, 1, 89, 3191, 4, 1, 210, 107, 8, 1, 423, 1022, 59, 910, 3822, 5, 1307, 8, 328, 982, 1170, 34, 10, 4703, 2460, 123, 84, 127, 1171, 114, 75, 309, 157, 9036, 1831, 18, 22, 32, 80, 7, 7, 2460, 117, 24, 125, 2, 227, 96, 1113, 14, 230, 5, 1172, 10, 683, 289, 1, 9037, 1304, 259, 154, 375, 2, 9038, 35, 92, 9039, 519, 849, 35, 2460, 1, 267, 4, 1, 2001, 204, 21, 1694, 71, 45, 786, 9040, 431, 6150, 12, 6, 1578, 4, 1, 644, 361, 8, 882, 107, 4, 1, 461, 2200, 7, 7, 29, 3, 569, 11, 103, 42, 6151, 984, 4704, 9041, 4, 9042, 12, 10, 6, 607, 35, 18, 1388, 26, 226, 4705, 5, 168, 3, 89, 883, 4, 4706, 8, 1, 1832, 4707, 7, 7, 137, 47, 21, 247, 114, 107, 44, 47, 17, 2460, 2757, 29, 24, 125, 1693, 97, 44, 2, 227, 10, 3, 3823, 9043, 947, 23, 162, 1, 9044, 4, 1, 9045, 9046, 95], [102, 342, 487, 6152, 125, 310, 2758, 2, 6153, 1833, 5, 220, 3, 1389, 5, 4708, 2201, 31, 21, 47, 31, 1066, 3, 1306, 1834, 254, 627, 1067, 102, 4709, 45, 55, 17, 1067, 28, 1173, 97, 43, 31, 162, 5, 220, 3, 4710, 1389, 5, 3192, 2759, 17, 96, 31, 787, 29, 1, 4711, 146, 31, 21, 84, 6154, 14, 6155, 1579, 2, 90, 84, 3193, 14, 6156, 172, 8, 3, 9047, 1484, 14, 135, 31, 129, 31, 884, 249, 22, 62, 1695, 48, 5, 76, 31, 118, 9048, 6157, 3194, 33, 256, 4712, 37, 440, 8, 4708, 17, 24, 331, 451, 46, 9, 12, 43, 23, 25, 1, 368, 28, 68, 375, 23, 177, 316, 4, 72, 18, 31, 137, 200, 84, 44, 7, 7, 1, 15, 6, 62, 51, 79, 9, 154, 403, 20, 766, 150, 5, 645, 2, 42, 235, 30, 1, 55, 11, 408, 251, 1, 317, 146, 9, 13, 27, 4, 141, 6158, 109, 23, 98, 121, 1, 1696, 744, 18, 23, 137, 215, 4, 121, 48, 62, 628, 17, 1067, 2, 1, 1485, 6155, 79, 4, 1, 9049, 6159, 462, 7, 7, 1308, 1697, 2, 1580, 4713, 178, 227, 161, 51, 341, 120, 6160, 279, 13, 3, 119, 642, 146, 178, 1697, 2, 6160, 116, 1174, 6, 51, 18, 101, 4, 463, 3824, 581, 4714, 56, 46, 3, 376, 3825, 224, 120, 7, 7, 477, 11, 198, 6161, 3195, 13, 3, 51, 15, 2, 11, 227, 9, 3, 745, 147], [10, 1581, 19, 13, 27, 4, 1, 125, 107, 29, 1, 2760, 4715, 19, 1835, 12, 11, 25, 131, 112, 47, 11, 452, 9, 47, 21, 36, 108, 177, 12, 13, 89, 38, 1, 19, 20, 355, 4, 30, 12, 1, 204, 2, 1114, 12, 11, 66, 1, 1486, 5, 1066, 59, 421, 4716, 11, 198, 12, 9050, 123, 3, 89, 332, 8, 24, 224, 2, 4717, 9051, 13, 421, 216, 5, 24, 224, 14, 3, 2002, 278, 11, 13, 2202, 29, 24, 985, 5, 26, 432, 5, 138, 35, 667, 1582, 252, 5, 136, 33, 1175, 224, 17, 57, 582, 1, 985, 8, 1, 19, 13, 41, 746, 1, 520, 13, 41, 89, 43, 23, 162, 5, 65, 33, 1581, 19, 10, 6, 27, 62, 12, 23, 133, 65, 11, 103, 12, 464, 9052, 64, 25, 83, 36, 2003, 5, 25, 136, 3, 9053, 8, 24, 1835, 2, 24, 788, 452, 1, 15, 36, 80, 53, 9, 1309, 1, 290, 712, 31, 488, 2, 222, 9, 187, 10, 409, 13, 3, 89, 19, 9, 13, 550, 2, 153, 2, 743, 2, 409, 1021, 9, 13, 41, 2761, 11, 288, 1115, 41, 36, 9054, 32, 10, 19, 2, 11, 62, 98, 162, 5, 2762, 9, 14, 92, 27, 41, 65, 9, 2, 409, 26, 2202, 29, 9, 11, 103, 12, 132, 19, 911, 62, 25, 48, 9, 276, 5, 138, 1384, 2, 11, 441, 5, 65, 49, 174, 35, 97, 8, 1, 684]]\n",
            "Tokenizer: \n",
            "# of unique tokens:  17721\n",
            "[(1, 'the'), (2, 'and'), (3, 'a'), (4, 'of'), (5, 'to'), (6, 'is'), (7, 'br'), (8, 'in'), (9, 'it'), (10, 'this')]\n",
            "[(17712, 'creator'), (17713, 'mated'), (17714, 'puppies'), (17715, 'trump'), (17716, 'festive'), (17717, 'canine'), (17718, 'demonic'), (17719, 'pooch'), (17720, 'biter'), (17721, 'revisiting')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xuAgKk4De2zK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "b19ff02c-2944-41f5-81ab-50a1fe728692"
      },
      "cell_type": "code",
      "source": [
        "# Pad the sequences to ensure constant length\n",
        "train_X = pad_sequences(train_X, maxlen=MAX_LEN)\n",
        "valid_X = pad_sequences(valid_X, maxlen=MAX_LEN)\n",
        "\n",
        "print(train_X)\n",
        "print(valid_X)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 101   25   83  114 ...    3  982    4 1063]\n",
            " [  10  209 6144 6141 ...   15  138   65    9]\n",
            " [   4  248  477   16 ...    1 9045 9046   95]\n",
            " [2759   17   96   31 ...    9    3  745  147]\n",
            " ...\n",
            " [   4   24  107    2 ...    2  452    6  573]\n",
            " [   0    0    0    0 ...    2  726  411  711]\n",
            " [  66   83  643    8 ...   39   24 5576 4320]\n",
            " [   0    0    0    0 ...    7   67  982 1291]]\n",
            "[[    0     0     0     0 ...     6     3  6633  5036]\n",
            " [    0     0     0     0 ...  4292     9   779  1291]\n",
            " [11724   236    58    22 ...    74   129  6401   839]\n",
            " [    0     0     0     0 ...    70   552   819   147]\n",
            " ...\n",
            " [   79     4     9    18 ...   107   500    10  4205]\n",
            " [    1   393    63    46 ...   127    32     1   118]\n",
            " [    8    63     1   532 ...     9   745     4   147]\n",
            " [    0     0     0     0 ...    32     1  6428  1272]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BMcTMa4le2zT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5bd0afcf-fb0f-4ea8-8a32-a1f917041ed5"
      },
      "cell_type": "code",
      "source": [
        "# Get the target values\n",
        "train_Y = train_df['label'].values\n",
        "valid_Y = valid_df['label'].values\n",
        "\n",
        "print(train_Y)\n",
        "print(valid_Y)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 0 1 ... 0 0 1 0]\n",
            "[1 0 1 1 ... 0 0 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A2msAxbHe2ze",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create input\n",
        "input = Input(shape=(MAX_LEN,), dtype='int32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1-mrO8vee2zo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Either use pretrained embeddings or create own embeddings from data\n",
        "\n",
        "# PRETRAINED\n",
        "# -----------------------------------\n",
        "# embeddings_index = {}\n",
        "# f = open('glove.6B.100d.txt',encoding='utf8')\n",
        "# for line in f:\n",
        "#     values = line.split()\n",
        "#     word = values[0]\n",
        "#     coefs = np.asarray(values[1:], dtype='float32')\n",
        "#     embeddings_index[word] = coefs\n",
        "# f.close()\n",
        "# \n",
        "# print('Total %s word vectors in Glove 6B 100d.' % len(embeddings_index))\n",
        "# \n",
        "# embedding_matrix = np.random.random((len(word_index) + 1, EMBED_SIZE))\n",
        "# for word, i in word_index.items():\n",
        "#     embedding_vector = embeddings_index.get(word)\n",
        "#     if embedding_vector is not None:\n",
        "#         # words not found in embedding index will be all-zeros.\n",
        "#         embedding_matrix[i] = embedding_vector\n",
        "# \n",
        "# embedding_layer = Embedding(len(word_index) + 1, EMBED_SIZE, weights=[embedding_matrix], input_length=MAX_LEN, trainable=True)\n",
        "# embedded_sequences = embedding_layer(input)\n",
        "\n",
        "# Create own embeddings\n",
        "# -----------------------------------\n",
        "embedded_sequences = Embedding(MAX_FEATURES, EMBED_SIZE)(input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qwhHWUjxe2zy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "25966ccc-4dff-49b3-815b-93582e50ee4b"
      },
      "cell_type": "code",
      "source": [
        "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
        "x = MaxPooling1D(4)(x)\n",
        "x = Conv1D(128, 5, activation='relu')(x)\n",
        "x = MaxPooling1D(5)(x)\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "\n",
        "preds = Dense(NUMBER_OF_CLASSES, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(input, preds)\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
        "model.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      (None, 200, 100)          1200000   \n",
            "_________________________________________________________________\n",
            "conv1d_6 (Conv1D)            (None, 196, 128)          64128     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1 (None, 49, 128)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_7 (Conv1D)            (None, 45, 128)           82048     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_6 (MaxPooling1 (None, 9, 128)            0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_3 ( (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 32)                4128      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,350,337\n",
            "Trainable params: 1,350,337\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VWE_KDpne2z9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "outputId": "ed74be3f-e729-4ae8-8006-5bcb05dd6b06"
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "checkpoint=ModelCheckpoint('model_cnn.hdf5',monitor='val_acc',verbose=1,save_best_only=True)\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, factor=0.5, min_lr=0.00001)\n",
        "history=model.fit(train_X, train_Y, validation_data=(valid_X, valid_Y), epochs=10, batch_size=2, callbacks=[checkpoint, learning_rate_reduction])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 799 samples, validate on 201 samples\n",
            "Epoch 1/10\n",
            "799/799 [==============================] - 11s 13ms/step - loss: 0.6952 - acc: 0.4956 - val_loss: 0.6720 - val_acc: 0.5572\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.55721, saving model to model_cnn.hdf5\n",
            "Epoch 2/10\n",
            "799/799 [==============================] - 11s 13ms/step - loss: 0.4992 - acc: 0.7584 - val_loss: 0.4796 - val_acc: 0.7960\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.55721 to 0.79602, saving model to model_cnn.hdf5\n",
            "Epoch 3/10\n",
            "799/799 [==============================] - 11s 13ms/step - loss: 0.2123 - acc: 0.9174 - val_loss: 0.5285 - val_acc: 0.8109\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.79602 to 0.81095, saving model to model_cnn.hdf5\n",
            "Epoch 4/10\n",
            "799/799 [==============================] - 11s 13ms/step - loss: 0.0746 - acc: 0.9787 - val_loss: 1.7719 - val_acc: 0.7214\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.81095\n",
            "Epoch 5/10\n",
            "799/799 [==============================] - 11s 14ms/step - loss: 0.0264 - acc: 0.9937 - val_loss: 1.5586 - val_acc: 0.8109\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.81095\n",
            "Epoch 6/10\n",
            "799/799 [==============================] - 10s 13ms/step - loss: 0.0190 - acc: 0.9950 - val_loss: 1.7161 - val_acc: 0.7811\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.81095\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 7/10\n",
            "799/799 [==============================] - 11s 13ms/step - loss: 6.0136e-06 - acc: 1.0000 - val_loss: 1.8811 - val_acc: 0.7662\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.81095\n",
            "Epoch 8/10\n",
            "799/799 [==============================] - 11s 13ms/step - loss: 2.5606e-07 - acc: 1.0000 - val_loss: 2.0452 - val_acc: 0.7861\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.81095\n",
            "Epoch 9/10\n",
            "799/799 [==============================] - 10s 13ms/step - loss: 1.2332e-07 - acc: 1.0000 - val_loss: 2.0797 - val_acc: 0.7861\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.81095\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "Epoch 10/10\n",
            "799/799 [==============================] - 11s 13ms/step - loss: 1.1223e-07 - acc: 1.0000 - val_loss: 2.0803 - val_acc: 0.7711\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.81095\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d62XbTB3e20F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig1 = plt.figure()\n",
        "plt.plot(history.history['loss'],'r',linewidth=3.0)\n",
        "plt.plot(history.history['val_loss'],'b',linewidth=3.0)\n",
        "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
        "plt.xlabel('Epochs ',fontsize=16)\n",
        "plt.ylabel('Loss',fontsize=16)\n",
        "plt.title('Loss Curves :CNN',fontsize=16)\n",
        "fig1.savefig('loss_cnn.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EhMkZCtpe20P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig2=plt.figure()\n",
        "plt.plot(history.history['acc'],'r',linewidth=3.0)\n",
        "plt.plot(history.history['val_acc'],'b',linewidth=3.0)\n",
        "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n",
        "plt.xlabel('Epochs ',fontsize=16)\n",
        "plt.ylabel('Accuracy',fontsize=16)\n",
        "plt.title('Accuracy Curves : CNN',fontsize=16)\n",
        "fig2.savefig('accuracy_cnn.png')\n",
        "plt.show()  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}